{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LSTM from scratch\n",
    "\n",
    "Sturcture of LSTM: https://www.pluralsight.com/guides/introduction-to-lstm-units-in-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np               \n",
    "import pandas as pd              \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "path = r'input/US_Baby_Names/NationalNames.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "#get names from the dataset\n",
    "data['Name'] = data['Name']\n",
    "\n",
    "#get first 10000 names\n",
    "data = np.array(data['Name'][:10000]).reshape(-1,1)\n",
    "\n",
    "#covert the names to lowee case\n",
    "data = [x.lower() for x in data[:,0]]\n",
    "\n",
    "data = np.array(data).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape = (10000, 1)\n",
      "[['anna']\n",
      " ['emma']\n",
      " ['elizabeth']\n",
      " ['minnie']\n",
      " ['margaret']\n",
      " ['ida']\n",
      " ['alice']\n",
      " ['bertha']\n",
      " ['sarah']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Shape = {}\".format(data.shape))\n",
    "print(data[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to store the transform data\n",
    "transform_data = np.copy(data)\n",
    "\n",
    "#find the max length name\n",
    "max_length = 0\n",
    "for index in range(len(data)):\n",
    "    max_length = max(max_length,len(data[index,0]))\n",
    "\n",
    "#make every name of max length by adding '.'\n",
    "for index in range(len(data)):\n",
    "    length = (max_length - len(data[index,0]))\n",
    "    string = '.'*length\n",
    "    transform_data[index,0] = ''.join([transform_data[index,0],string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data\n",
      "[['anna........']\n",
      " ['emma........']\n",
      " ['elizabeth...']\n",
      " ['minnie......']\n",
      " ['margaret....']\n",
      " ['ida.........']\n",
      " ['alice.......']\n",
      " ['bertha......']\n",
      " ['sarah.......']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed Data\")\n",
    "print(transform_data[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 27\n",
      "Vocab      = {'h', 'm', 'z', 'y', 'n', 'q', 'o', 't', 's', 'b', 'u', '.', 'r', 'w', 'a', 'v', 'g', 'f', 'e', 'p', 'l', 'j', 'i', 'x', 'k', 'd', 'c'}\n"
     ]
    }
   ],
   "source": [
    "#to store the vocabulary\n",
    "vocab = list()\n",
    "for name in transform_data[:,0]:\n",
    "    vocab.extend(list(name))\n",
    "\n",
    "vocab = set(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocab size = {}\".format(len(vocab)))\n",
    "print(\"Vocab      = {}\".format(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-14, 22-i\n"
     ]
    }
   ],
   "source": [
    "#map char to id and id to chars\n",
    "char_id = dict()\n",
    "id_char = dict()\n",
    "\n",
    "for i,char in enumerate(vocab):\n",
    "    char_id[char] = i\n",
    "    id_char[i] = char\n",
    "\n",
    "print('a-{}, 22-{}'.format(char_id['a'],id_char[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a training set \n",
    "\n",
    "Example - names - [['mary.'], ['anna.']\n",
    "\n",
    "m - [0,0,0,1,0,0]\n",
    "\n",
    "a - [0,0,1,0,0,0]\n",
    "\n",
    "r - [0,1,0,0,0,0]\n",
    "\n",
    "y - [0,0,0,0,1,0]\n",
    "\n",
    ". - [1,0,0,0,0,0]\n",
    "\n",
    "'mary.' = [[0,0,0,1,0,0], [0,0,1,0,0,0], [0,1,0,0,0,0], [0,0,0,0,1,0], [1,0,0,0,0,0]]\n",
    "\n",
    "'anna.' = [[0,0,1,0,0,0], [0,0,0,0,0,1], [0,0,0,0,0,1], [0,0,1,0,0,0], [1,0,0,0,0,0]]\n",
    "\n",
    "batch_dataset = [ [[0,0,0,1,0,0],[0,0,1,0,0,0]] , [[0,0,1,0,0,0], [0,0,0,0,0,1]], [[0,1,0,0,0,0], [0,0,0,0,0,1]], [[0,0,0,0,1,0], [0,0,1,0,0,0]] , [ [1,0,0,0,0,0], [1,0,0,0,0,0]] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of batches of size = 20\n",
    "train_dataset = []\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "#split the trasnform data into batches of 20\n",
    "for i in range(len(transform_data)-batch_size+1):\n",
    "    start = i*batch_size\n",
    "    end = start+batch_size\n",
    "    \n",
    "    #batch data\n",
    "    batch_data = transform_data[start:end]\n",
    "    \n",
    "    if(len(batch_data)!=batch_size):\n",
    "        break\n",
    "        \n",
    "    #convert each char of each name of batch data into one hot encoding\n",
    "    char_list = []\n",
    "    for k in range(len(batch_data[0][0])):#enumerate over length of name\n",
    "        batch_dataset = np.zeros([batch_size,len(vocab)])\n",
    "        for j in range(batch_size):# enumerate over a batch\n",
    "            name = batch_data[j][0]\n",
    "            char_index = char_id[name[k]]\n",
    "            batch_dataset[j,char_index] = 1.0 #order and grouped by columns \n",
    "     \n",
    "        #store the ith char's one hot representation of each name in batch_data\n",
    "        char_list.append(batch_dataset)\n",
    "    \n",
    "    #store each char's of every name in batch dataset into train_dataset\n",
    "    train_dataset.append(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input units or embedding size\n",
    "input_units = 100\n",
    "\n",
    "#number of hidden neurons\n",
    "hidden_units = 256\n",
    "\n",
    "#number of output units i.e vocab size\n",
    "output_units = vocab_size\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "#beta1 for V parameters used in Adam Optimizer\n",
    "beta1 = 0.90\n",
    "\n",
    "#beta2 for S parameters used in Adam Optimizer\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "#sigmoid\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "#tanh activation\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "#softmax activation\n",
    "#Softmax = exp(X)/(sum(exp(X),1))\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n",
    "    exp_X = exp_X/exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "#derivative of tanh\n",
    "def tanh_derivative(X):\n",
    "    return 1-(X**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "def initialize_parameters():\n",
    "    #initialize the parameters with 0 mean and 0.01 standard deviation\n",
    "    mean = 0\n",
    "    std = 0.01\n",
    "    \n",
    "    #lstm cell weights\n",
    "    forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    \n",
    "    #hidden to output weights (output cell)\n",
    "    hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))\n",
    "    \n",
    "    parameters = dict()\n",
    "    parameters['fgw'] = forget_gate_weights\n",
    "    parameters['igw'] = input_gate_weights\n",
    "    parameters['ogw'] = output_gate_weights\n",
    "    parameters['ggw'] = gate_gate_weights\n",
    "    parameters['how'] = hidden_output_weights\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    \n",
    "    #concat batch data and prev_activation matrix\n",
    "    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)\n",
    "    \n",
    "    #forget gate activations\n",
    "    fa = np.matmul(concat_dataset,fgw)\n",
    "    fa = sigmoid(fa)\n",
    "    \n",
    "    #input gate activations\n",
    "    ia = np.matmul(concat_dataset,igw)\n",
    "    ia = sigmoid(ia)\n",
    "    \n",
    "    #output gate activations\n",
    "    oa = np.matmul(concat_dataset,ogw)\n",
    "    oa = sigmoid(oa)\n",
    "    \n",
    "    #gate gate activations\n",
    "    ga = np.matmul(concat_dataset,ggw)\n",
    "    ga = tanh_activation(ga)\n",
    "    \n",
    "    #new cell memory matrix\n",
    "    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)\n",
    "    \n",
    "    #current activation matrix\n",
    "    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))\n",
    "    \n",
    "    #lets store the activations to be used in back prop\n",
    "    lstm_activations = dict()\n",
    "    lstm_activations['fa'] = fa\n",
    "    lstm_activations['ia'] = ia\n",
    "    lstm_activations['oa'] = oa\n",
    "    lstm_activations['ga'] = ga\n",
    "    \n",
    "    return lstm_activations,cell_memory_matrix,activation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_cell(activation_matrix,parameters):\n",
    "    #get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get outputs \n",
    "    output_matrix = np.matmul(activation_matrix,how)\n",
    "    output_matrix = softmax(output_matrix)\n",
    "    \n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(batch_dataset,embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset,embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(batches,parameters,embeddings):\n",
    "    #get batch size\n",
    "    batch_size = batches[0].shape[0]\n",
    "    \n",
    "    #to store the activations of all the unrollings.\n",
    "    lstm_cache = dict()                 #lstm cache\n",
    "    activation_cache = dict()           #activation cache \n",
    "    cell_cache = dict()                 #cell cache\n",
    "    output_cache = dict()               #output cache\n",
    "    embedding_cache = dict()            #embedding cache \n",
    "    \n",
    "    #initial activation_matrix(a0) and cell_matrix(c0)\n",
    "    a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    \n",
    "    #store the initial activations in cache\n",
    "    activation_cache['a0'] = a0\n",
    "    cell_cache['c0'] = c0\n",
    "    \n",
    "    #unroll the names\n",
    "    for i in range(len(batches)-1):\n",
    "        #get first first character batch\n",
    "        batch_dataset = batches[i]\n",
    "        \n",
    "        #get embeddings \n",
    "        batch_dataset = get_embeddings(batch_dataset,embeddings)\n",
    "        embedding_cache['emb'+str(i)] = batch_dataset\n",
    "        \n",
    "        #lstm cell\n",
    "        lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n",
    "        \n",
    "        #output cell\n",
    "        ot = output_cell(at,parameters)\n",
    "        \n",
    "        #store the time 't' activations in caches\n",
    "        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n",
    "        activation_cache['a'+str(i+1)] = at\n",
    "        cell_cache['c' + str(i+1)] = ct\n",
    "        output_cache['o'+str(i+1)] = ot\n",
    "        \n",
    "        #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "        a0 = at\n",
    "        c0 = ct\n",
    "        \n",
    "    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss\n",
    "\n",
    "Loss at time t = -sum(Y x log(d) + (1-Y) x log(1-pred)))/m\n",
    "\n",
    "Overall Loss = ∑(Loss(t)) sum of all losses at each time step 't'\n",
    "\n",
    "##### Perplexity\n",
    "\n",
    "Probability Product = ∏(prob(pred_char)) for each char in name\n",
    "\n",
    "Perplexity = (1/probability_product) ^ (1/n) where n in number of chars in name\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "Accuracy(t) = (Y==predictions,axis=1) for all time steps\n",
    "\n",
    "Accuracy = ((∑Acc(t))/batch_size)/n for all time steps, n is number of chars in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss, perplexity and accuracy\n",
    "def cal_loss_accuracy(batch_labels,output_cache):\n",
    "    loss = 0  #to sum loss for each time step\n",
    "    acc  = 0  #to sum acc for each time step \n",
    "    prob = 1  #probability product of each time step predicted char\n",
    "    \n",
    "    #batch size\n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    #loop through each time step\n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        #get true labels and predictions\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))\n",
    "        loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)\n",
    "        acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)\n",
    "    \n",
    "    #calculate perplexity loss and accuracy\n",
    "    perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size\n",
    "    loss = np.sum(loss)*(-1/batch_size)\n",
    "    acc  = np.sum(acc)/(batch_size)\n",
    "    acc = acc/len(output_cache)\n",
    "    \n",
    "    return perplexity,loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Output Cell Errors for each time step\n",
    "Output Error Cache :- to store output error for each time step\n",
    "\n",
    "Activation Error Cache : to store activation error for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate output cell errors\n",
    "def calculate_output_cell_error(batch_labels,output_cache,parameters):\n",
    "    #to store the output errors for each time step\n",
    "    output_error_cache = dict()\n",
    "    activation_error_cache = dict()\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #loop through each time step\n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        #get true and predicted labels\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        #calculate the output_error for time step 't'\n",
    "        error_output = pred - labels\n",
    "        \n",
    "        #calculate the activation error for time step 't'\n",
    "        error_activation = np.matmul(error_output,how.T)\n",
    "        \n",
    "        #store the output and activation error in dict\n",
    "        output_error_cache['eo'+str(i)] = error_output\n",
    "        activation_error_cache['ea'+str(i)] = error_activation\n",
    "        \n",
    "    return output_error_cache,activation_error_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate error for single lstm cell\n",
    "def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):\n",
    "    #activation error =  error coming from output cell and error coming from the next lstm cell\n",
    "    activation_error = activation_output_error + next_activation_error\n",
    "    \n",
    "    #output gate error\n",
    "    oa = lstm_activation['oa']\n",
    "    eo = np.multiply(activation_error,tanh_activation(cell_activation))\n",
    "    eo = np.multiply(np.multiply(eo,oa),1-oa)\n",
    "    \n",
    "    #cell activation error\n",
    "    cell_error = np.multiply(activation_error,oa)\n",
    "    cell_error = np.multiply(cell_error,tanh_derivative(tanh_activation(cell_activation)))\n",
    "    #error also coming from next lstm cell \n",
    "    cell_error += next_cell_error\n",
    "    \n",
    "    #input gate error\n",
    "    ia = lstm_activation['ia']\n",
    "    ga = lstm_activation['ga']\n",
    "    ei = np.multiply(cell_error,ga)\n",
    "    ei = np.multiply(np.multiply(ei,ia),1-ia)\n",
    "    \n",
    "    #gate gate error\n",
    "    eg = np.multiply(cell_error,ia)\n",
    "    eg = np.multiply(eg,tanh_derivative(ga))\n",
    "    \n",
    "    #forget gate error\n",
    "    fa = lstm_activation['fa']\n",
    "    ef = np.multiply(cell_error,prev_cell_activation)\n",
    "    ef = np.multiply(np.multiply(ef,fa),1-fa)\n",
    "    \n",
    "    #prev cell error\n",
    "    prev_cell_error = np.multiply(cell_error,fa)\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ggw = parameters['ggw']\n",
    "    ogw = parameters['ogw']\n",
    "    \n",
    "    #embedding + hidden activation error\n",
    "    embed_activation_error = np.matmul(ef,fgw.T)\n",
    "    embed_activation_error += np.matmul(ei,igw.T)\n",
    "    embed_activation_error += np.matmul(eo,ogw.T)\n",
    "    embed_activation_error += np.matmul(eg,ggw.T)\n",
    "    \n",
    "    input_hidden_units = fgw.shape[0]\n",
    "    hidden_units = fgw.shape[1]\n",
    "    input_units = input_hidden_units - hidden_units\n",
    "    \n",
    "    #prev activation error\n",
    "    prev_activation_error = embed_activation_error[:,input_units:]\n",
    "    \n",
    "    #input error (embedding error)\n",
    "    embed_error = embed_activation_error[:,:input_units]\n",
    "    \n",
    "    #store lstm error\n",
    "    lstm_error = dict()\n",
    "    lstm_error['ef'] = ef\n",
    "    lstm_error['ei'] = ei\n",
    "    lstm_error['eo'] = eo\n",
    "    lstm_error['eg'] = eg\n",
    "    \n",
    "    return prev_activation_error,prev_cell_error,embed_error,lstm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Output Cell Derivatives for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate output cell derivatives\n",
    "def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):\n",
    "    #to store the sum of derivatives from each time step\n",
    "    dhow = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    batch_size = activation_cache['a1'].shape[0]\n",
    "    \n",
    "    #loop through the time steps \n",
    "    for i in range(1,len(output_error_cache)+1):\n",
    "        #get output error\n",
    "        output_error = output_error_cache['eo' + str(i)]\n",
    "        \n",
    "        #get input activation\n",
    "        activation = activation_cache['a'+str(i)]\n",
    "        \n",
    "        #cal derivative and summing up\n",
    "        dhow += np.matmul(activation.T,output_error)/batch_size\n",
    "        \n",
    "    return dhow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate LSTM CELL Derivatives for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate derivatives for single lstm cell\n",
    "def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):\n",
    "    #get error for single time step\n",
    "    ef = lstm_error['ef']\n",
    "    ei = lstm_error['ei']\n",
    "    eo = lstm_error['eo']\n",
    "    eg = lstm_error['eg']\n",
    "    \n",
    "    #get input activations for this time step\n",
    "    concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)\n",
    "    \n",
    "    batch_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    #cal derivatives for this time step\n",
    "    dfgw = np.matmul(concat_matrix.T,ef)/batch_size\n",
    "    digw = np.matmul(concat_matrix.T,ei)/batch_size\n",
    "    dogw = np.matmul(concat_matrix.T,eo)/batch_size\n",
    "    dggw = np.matmul(concat_matrix.T,eg)/batch_size\n",
    "    \n",
    "    #store the derivatives for this time step in dict\n",
    "    derivatives = dict()\n",
    "    derivatives['dfgw'] = dfgw\n",
    "    derivatives['digw'] = digw\n",
    "    derivatives['dogw'] = dogw\n",
    "    derivatives['dggw'] = dggw\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backpropagation\n",
    "Apply chain rule and calculate the errors for each time step\n",
    "\n",
    "Store the deivatives in derivatives dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):\n",
    "    #calculate output errors \n",
    "    output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)\n",
    "    \n",
    "    #to store lstm error for each time step\n",
    "    lstm_error_cache = dict()\n",
    "    \n",
    "    #to store embeding errors for each time step\n",
    "    embedding_error_cache = dict()\n",
    "    \n",
    "    # next activation error \n",
    "    # next cell error  \n",
    "    #for last cell will be zero\n",
    "    eat = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    ect = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    \n",
    "    #calculate all lstm cell errors (going from last time-step to the first time step)\n",
    "    for i in range(len(lstm_cache),0,-1):\n",
    "        #calculate the lstm errors for this time step 't'\n",
    "        pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])\n",
    "        \n",
    "        #store the lstm error in dict\n",
    "        lstm_error_cache['elstm'+str(i)] = le\n",
    "        \n",
    "        #store the embedding error in dict\n",
    "        embedding_error_cache['eemb'+str(i-1)] = ee\n",
    "        \n",
    "        #update the next activation error and next cell error for previous cell\n",
    "        eat = pae\n",
    "        ect = pce\n",
    "    \n",
    "    \n",
    "    #calculate output cell derivatives\n",
    "    derivatives = dict()\n",
    "    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)\n",
    "    \n",
    "    #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict\n",
    "    lstm_derivatives = dict()\n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])\n",
    "    \n",
    "    #initialize the derivatives to zeros \n",
    "    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n",
    "    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    \n",
    "    #sum up the derivatives for each time step\n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n",
    "        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n",
    "        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n",
    "        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n",
    "    \n",
    "    return derivatives,embedding_error_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam\n",
    "Using Exponentially Weighted Averages\n",
    "\n",
    "Vdw = beta1 x Vdw + (1-beta1) x (dw)\n",
    "\n",
    "Sdw = beta2 x Sdw + (1-beta2) x dw^2\n",
    "\n",
    "W = W - learning_rate x ( Vdw/ (sqrt(Sdw)+1e-7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the parameters using adam optimizer\n",
    "#adam optimization\n",
    "def update_parameters(parameters,derivatives,V,S,t):\n",
    "    #get derivatives\n",
    "    dfgw = derivatives['dfgw']\n",
    "    digw = derivatives['digw']\n",
    "    dogw = derivatives['dogw']\n",
    "    dggw = derivatives['dggw']\n",
    "    dhow = derivatives['dhow']\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get V parameters\n",
    "    vfgw = V['vfgw']\n",
    "    vigw = V['vigw']\n",
    "    vogw = V['vogw']\n",
    "    vggw = V['vggw']\n",
    "    vhow = V['vhow']\n",
    "    \n",
    "    #get S parameters\n",
    "    sfgw = S['sfgw']\n",
    "    sigw = S['sigw']\n",
    "    sogw = S['sogw']\n",
    "    sggw = S['sggw']\n",
    "    show = S['show']\n",
    "    \n",
    "    #calculate the V parameters from V and current derivatives\n",
    "    vfgw = (beta1*vfgw + (1-beta1)*dfgw)\n",
    "    vigw = (beta1*vigw + (1-beta1)*digw)\n",
    "    vogw = (beta1*vogw + (1-beta1)*dogw)\n",
    "    vggw = (beta1*vggw + (1-beta1)*dggw)\n",
    "    vhow = (beta1*vhow + (1-beta1)*dhow)\n",
    "    \n",
    "    #calculate the S parameters from S and current derivatives\n",
    "    sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))\n",
    "    sigw = (beta2*sigw + (1-beta2)*(digw**2))\n",
    "    sogw = (beta2*sogw + (1-beta2)*(dogw**2))\n",
    "    sggw = (beta2*sggw + (1-beta2)*(dggw**2))\n",
    "    show = (beta2*show + (1-beta2)*(dhow**2))\n",
    "    \n",
    "    #update the parameters\n",
    "    fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))\n",
    "    igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))\n",
    "    ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))\n",
    "    ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))\n",
    "    how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))\n",
    "    \n",
    "    #store the new weights\n",
    "    parameters['fgw'] = fgw\n",
    "    parameters['igw'] = igw\n",
    "    parameters['ogw'] = ogw\n",
    "    parameters['ggw'] = ggw\n",
    "    parameters['how'] = how\n",
    "    \n",
    "    #store the new V parameters\n",
    "    V['vfgw'] = vfgw \n",
    "    V['vigw'] = vigw \n",
    "    V['vogw'] = vogw \n",
    "    V['vggw'] = vggw\n",
    "    V['vhow'] = vhow\n",
    "    \n",
    "    #store the s parameters\n",
    "    S['sfgw'] = sfgw \n",
    "    S['sigw'] = sigw \n",
    "    S['sogw'] = sogw \n",
    "    S['sggw'] = sggw\n",
    "    S['show'] = show\n",
    "    \n",
    "    return parameters,V,S  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the Embeddings\n",
    "def update_embeddings(embeddings,embedding_error_cache,batch_labels):\n",
    "    #to store the embeddings derivatives\n",
    "    embedding_derivatives = np.zeros(embeddings.shape)\n",
    "    \n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    #sum the embedding derivatives for each time step\n",
    "    for i in range(len(embedding_error_cache)):\n",
    "        embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size\n",
    "    \n",
    "    #update the embeddings\n",
    "    embeddings = embeddings - learning_rate*embedding_derivatives\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_V(parameters):\n",
    "    Vfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Vigw = np.zeros(parameters['igw'].shape)\n",
    "    Vogw = np.zeros(parameters['ogw'].shape)\n",
    "    Vggw = np.zeros(parameters['ggw'].shape)\n",
    "    Vhow = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    V = dict()\n",
    "    V['vfgw'] = Vfgw\n",
    "    V['vigw'] = Vigw\n",
    "    V['vogw'] = Vogw\n",
    "    V['vggw'] = Vggw\n",
    "    V['vhow'] = Vhow\n",
    "    return V\n",
    "\n",
    "def initialize_S(parameters):\n",
    "    Sfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Sigw = np.zeros(parameters['igw'].shape)\n",
    "    Sogw = np.zeros(parameters['ogw'].shape)\n",
    "    Sggw = np.zeros(parameters['ggw'].shape)\n",
    "    Show = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    S = dict()\n",
    "    S['sfgw'] = Sfgw\n",
    "    S['sigw'] = Sigw\n",
    "    S['sogw'] = Sogw\n",
    "    S['sggw'] = Sggw\n",
    "    S['show'] = Show\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train functions\n",
    "1. Initialize Parameters\n",
    "2. Forward Propagation\n",
    "3. Calculate Loss, Perplexity and Accuracy\n",
    "4. Backward Propagation\n",
    "5. Update the Parameters and Embeddings\n",
    "\n",
    "Batch Size = 20 Repeat the steps 2-5 for each batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(train_dataset,iters=1000,batch_size=20):\n",
    "    #initalize the parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    #initialize the V and S parameters for Adam\n",
    "    V = initialize_V(parameters)\n",
    "    S = initialize_S(parameters)\n",
    "    \n",
    "    #generate the random embeddings\n",
    "    embeddings = np.random.normal(0,0.01,(len(vocab),input_units))\n",
    "    \n",
    "    #to store the Loss, Perplexity and Accuracy for each batch\n",
    "    J = []\n",
    "    P = []\n",
    "    A = []\n",
    "    \n",
    "    \n",
    "    for step in range(iters):\n",
    "        #get batch dataset\n",
    "        index = step%len(train_dataset)\n",
    "        batches = train_dataset[index]\n",
    "        \n",
    "        #forward propagation\n",
    "        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)\n",
    "        \n",
    "        #calculate the loss, perplexity and accuracy\n",
    "        perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)\n",
    "        \n",
    "        #backward propagation\n",
    "        derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)\n",
    "        \n",
    "        #update the parameters\n",
    "        parameters,V,S = update_parameters(parameters,derivatives,V,S,step)\n",
    "        \n",
    "        #update the embeddings\n",
    "        embeddings = update_embeddings(embeddings,embedding_error_cache,batches)\n",
    "        \n",
    "        \n",
    "        J.append(loss)\n",
    "        P.append(perplexity)\n",
    "        A.append(acc)\n",
    "        \n",
    "        #print loss, accuracy and perplexity\n",
    "        if(step%1000==0):\n",
    "            print(\"For Single Batch :\")\n",
    "            print('Step       = {}'.format(step))\n",
    "            print('Loss       = {}'.format(round(loss,2)))\n",
    "            print('Perplexity = {}'.format(round(perplexity,2)))\n",
    "            print('Accuracy   = {}'.format(round(acc*100,2)))\n",
    "            print()\n",
    "    \n",
    "    return embeddings, parameters,J,P,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Single Batch :\n",
      "Step       = 0\n",
      "Loss       = 47.05\n",
      "Perplexity = 27.0\n",
      "Accuracy   = 23.18\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 1000\n",
      "Loss       = 12.47\n",
      "Perplexity = 2.16\n",
      "Accuracy   = 74.09\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 2000\n",
      "Loss       = 8.89\n",
      "Perplexity = 1.67\n",
      "Accuracy   = 83.18\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 3000\n",
      "Loss       = 8.13\n",
      "Perplexity = 1.61\n",
      "Accuracy   = 82.27\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 4000\n",
      "Loss       = 8.15\n",
      "Perplexity = 1.6\n",
      "Accuracy   = 83.18\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 5000\n",
      "Loss       = 8.34\n",
      "Perplexity = 1.62\n",
      "Accuracy   = 83.64\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 6000\n",
      "Loss       = 7.89\n",
      "Perplexity = 1.57\n",
      "Accuracy   = 83.18\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 7000\n",
      "Loss       = 7.92\n",
      "Perplexity = 1.57\n",
      "Accuracy   = 83.64\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 8000\n",
      "Loss       = 7.94\n",
      "Perplexity = 1.57\n",
      "Accuracy   = 84.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings,parameters,J,P,A = train(train_dataset,iters=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "def predict(parameters,embeddings,id_char,vocab_size):\n",
    "    #to store some predicted names\n",
    "    names = []\n",
    "    \n",
    "    #predict 20 names\n",
    "    for i in range(20):\n",
    "        #initial activation_matrix(a0) and cell_matrix(c0)\n",
    "        a0 = np.zeros([1,hidden_units],dtype=np.float32)\n",
    "        c0 = np.zeros([1,hidden_units],dtype=np.float32)\n",
    "\n",
    "        #initalize blank name\n",
    "        name = ''\n",
    "        \n",
    "        #make a batch dataset of single char\n",
    "        batch_dataset = np.zeros([1,vocab_size])\n",
    "        \n",
    "        #get random start character\n",
    "        index = np.random.randint(0,27,1)[0]\n",
    "        \n",
    "        #make that index 1.0\n",
    "        batch_dataset[0,index] = 1.0\n",
    "        \n",
    "        #add first char to name\n",
    "        name += id_char[index]\n",
    "        \n",
    "        #get char from id_char dict\n",
    "        char = id_char[index]\n",
    "        \n",
    "        #loop until algo predicts '.'\n",
    "        while(char!='.'):\n",
    "            #get embeddings\n",
    "            batch_dataset = get_embeddings(batch_dataset,embeddings)\n",
    "\n",
    "            #lstm cell\n",
    "            lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n",
    "\n",
    "            #output cell\n",
    "            ot = output_cell(at,parameters)\n",
    "            \n",
    "            #either select random.choice ot np.argmax\n",
    "            pred = np.random.choice(27,1,p=ot[0])[0]\n",
    "            \n",
    "            #get predicted char index\n",
    "            #pred = np.argmax(ot)\n",
    "                \n",
    "            #add char to name\n",
    "            name += id_char[pred]\n",
    "            \n",
    "            char = id_char[pred]\n",
    "            \n",
    "            #change the batch_dataset to this new predicted char\n",
    "            batch_dataset = np.zeros([1,vocab_size])\n",
    "            batch_dataset[0,pred] = 1.0\n",
    "\n",
    "            #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "            a0 = at\n",
    "            c0 = ct\n",
    "            \n",
    "        #append the predicted name to names list\n",
    "        names.append(name)\n",
    "        \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gilbert.',\n",
       " 'nanna.',\n",
       " 'zelma.',\n",
       " 'curtis.',\n",
       " 'thelma.',\n",
       " 'nathan.',\n",
       " 'louis.',\n",
       " 'x.',\n",
       " 'x.',\n",
       " 'kathryne.',\n",
       " 'kathleen.',\n",
       " 'leonard.',\n",
       " 'rachael.',\n",
       " 'dorothea.',\n",
       " 'yatta.',\n",
       " 'xduis.',\n",
       " 'nena.',\n",
       " 'x.',\n",
       " 'van.',\n",
       " 'iva.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(parameters,embeddings,id_char,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
